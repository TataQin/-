https://mp.weixin.qq.com/s/YZaxf9GBvh_rT8hRx1XPbA

HTTP 1.0 是 1996 年发布的，奠定了 web 的基础。时隔三年，1999 年又发布了 HTTP 1.1，对功能上做了扩充。之后又时隔十六年，2015 年发布了 HTTP 2.0
HTTP 2.0 没有没有功能上的新增，只是优化了性能。

为什么要这么大的版本升级来优化性能，HTTP 1.1 的性能很差么？

# HTTP 1.1 的问题

我们知道，HTTP 的下层协议是 TCP，需要经历三次握手才能建立连接。而 HTTP 1.0 的时候一次请求和响应结束就会断开链接，这样下次请求又要重新三次握手来建立连接。

为了减少这种建立 TCP 链接的消耗，HTTP 1.1 支持了 keep-alive，只要请求或响应头带上 Connection: keep-alive，就可以告诉对方先不要断开链接，我之后还要用这个链接发消息。

当需要断开的时候，再指定 Connection: close 的 header。

这样就可以用同一个 TCP 链接进行多次 HTTP 请求响应了

但这样虽然减少了链接的建立，在性能上却有问题，下次请求得等上一个请求返回响应才能发出。

这个问题有个名字，叫做队头阻塞，很容易理解，因为多个请求要排队嘛，队前面的卡住了，那后面的也就执行不了了。

怎么解决这个问题呢？

HTTP 1.1 提出了管道的概念，就是多个请求可以并行发送，返回响应后再依次处理。

其实这样能部分解决问题，但是返回的响应依然要依次处理，解决不了队头阻塞的问题。

所以说管道化是比较鸡肋的一个功能，现在绝大多数浏览器都默认关闭了，甚至都不支持。

那还能怎么解决这个队头阻塞的问题呢？

开多个队不就行了。

浏览器一般会同一个域名建立 6-8 个 TCP 链接，也就是 6-8 个队，如果一个队发生队头阻塞了，那就放到其他的队里。

这样就缓解了队头阻塞问题。

除了队头阻塞的问题，HTTP 1.1 还有没有别的问题？

有，比如 header 部分太大了。

# http 2.0的优化

比如队头阻塞的问题，也就是第二个响应要等第一个响应处理完之后才能处理。怎么解决？

这个很容易解决呀，每个请求、响应都加上一个 ID，然后每个响应和通过 ID 来找到它对应的请求。各回各家，自然就不用阻塞的等待了。

再比如说 header 过大这个问题，怎么解决 ？

文本传输太占空间，换成二进制的是不是会好很多。

还有，每次传输都有很多相同的 header，能不能建立一张表，传的时候只传输下标就行了。

还有，body 可以压缩，那 header 是不是可以压缩。

这样处理之后，应该会好很多。

那没有充分利用 TCP 的能力，只支持请求--响应的方式呢？

那就支持服务端主动推送呀，但是客户端可以选择接收或者不接收。

HTTP2 确实是通过 ID 把请求和响应关联起来了，它把这个概念叫做流 stream。

而且我们之前说了 header 需要单独的优化嘛，所以把 header 和 body 部分分开来传送，叫做不同的帧 frame。

header 部分最开始是长度，然后是这个帧的类型，有这样几种类型：

- SETTINGS 帧：配置信息，比如最大帧 size，是否支持 server push 等。
- HEADERS 帧：请求或响应的 header
- DATA 帧：请求或响应的 body
- CONTINUATION 帧：一个帧不够装的时候，可以分帧，用这个可以引用上一个帧。
- PUSH_PROMISE 帧：服务端推送数据的帧
- END_STREAM 帧：表示流传输结束
- RST_STREAM 帧，用来终止当前流

HTTP2 确实是支持服务端推送的，这时候帧类型也是单独的，叫做 PUSH_PROMISE。

HTTP 2.0 解决了 1.1 的这些问题，通过多路复用，也就是请求和响应在一个流里，通过同一个流 id 来关联多个帧的方式来传输数据。多个流可以并发。

HTTP2性能提升的核心就在于二进制分帧层。HTTP2是二进制协议，他采用二进制格式传输数据而不是1.x的文本格式。

## 多路复用

也就是请求和响应在一个流里，通过同一个流 id 来关联多个帧的方式来传输数据。多个流可以并发。

HTTP2让所有的通信都在一个TCP连接上完成，真正实现了请求的并发。

HTTP2建立一个TCP连接，一个连接上面可以有任意多个流（stream），消息分割成一个或多个帧在流里面传输。帧传输过去以后，再进行重组，形成一个完整的请求或响应。

## 头部压缩

在1.1中，首部用文本格式传输，每个请求带的一些首部字段都是相同的，例如cookir、user-agent。HTTP 2.0 采用HPACK压缩格式来压缩首部

## 服务端推送